{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31192,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliawuxh/philosopher-style-classification/blob/main/nlp_research_note_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation & Imports"
      ],
      "metadata": {
        "id": "WPtptslqdPS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 1: INSTALL PACKAGES & AUTO-RESTART\n",
        "# ============================================================================\n",
        "\n",
        "!pip uninstall -y -q bertopic sentence-transformers umap-learn hdbscan scikit-learn\n",
        "!pip install -q \"scikit-learn==1.4.2\" \"umap-learn==0.5.6\" \"hdbscan==0.8.40\" \\\n",
        "             \"sentence-transformers==3.0.1\" \"bertopic==0.16.4\"\n",
        "!pip install -q transformers datasets evaluate\n",
        "\n",
        "print(\"✓ Packages installed\")\n",
        "print(\"⚠️ Restarting kernel automatically...\")\n",
        "\n",
        "# Auto-restart kernel\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T15:45:42.247353Z",
          "iopub.execute_input": "2026-01-02T15:45:42.248159Z",
          "execution_failed": "2026-01-02T15:47:55.752Z"
        },
        "id": "TnS-TnwudPS6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 2: IMPORTS & SETUP (Run after auto-restart)\n",
        "# ============================================================================\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
        "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import everything\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback,\n",
        ")\n",
        "import evaluate\n",
        "\n",
        "# BERTopic imports\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import hdbscan\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Random seed\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "torch.manual_seed(RANDOM_STATE)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"✓ SETUP COMPLETE\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"Scikit-learn: {__import__('sklearn').__version__}\")  # Should show 1.4.2\n",
        "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T15:48:09.040532Z",
          "iopub.execute_input": "2026-01-02T15:48:09.040883Z",
          "iopub.status.idle": "2026-01-02T15:48:56.504869Z",
          "shell.execute_reply.started": "2026-01-02T15:48:09.040856Z",
          "shell.execute_reply": "2026-01-02T15:48:56.503545Z"
        },
        "id": "npTB1gSCdPS7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "OUT_DIR = Path(\"gutenberg_texts\")\n",
        "OUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "BOOK_URLS = [\n",
        "    # Nietzsche\n",
        "    (\"Nietzsche\", \"Thus_Spake_Zarathustra\", \"https://www.gutenberg.org/cache/epub/1998/pg1998.txt\"),\n",
        "    (\"Nietzsche\", \"Beyond_Good_and_Evil\", \"https://www.gutenberg.org/cache/epub/4363/pg4363.txt\"),\n",
        "    (\"Nietzsche\", \"Genealogy_of_Morals\", \"https://www.gutenberg.org/cache/epub/52319/pg52319.txt\"),\n",
        "    (\"Nietzsche\", \"The Twilight of the Idols; or, How to Philosophize with the Hammer\", \"https://www.gutenberg.org/cache/epub/52263/pg52263.txt\"),\n",
        "    (\"Nietzsche\", \"The Birth of Tragedy; or, Hellenism and Pessimism\", \"https://www.gutenberg.org/cache/epub/51356/pg51356.txt\"),\n",
        "    (\"Nietzsche\", \"The Dawn of Day\", \"https://www.gutenberg.org/cache/epub/39955/pg39955.txt\"),\n",
        "\n",
        "    # Kant\n",
        "    (\"Kant\", \"Critique_of_Pure_Reason\", \"https://www.gutenberg.org/cache/epub/4280/pg4280.txt\"),\n",
        "    (\"Kant\", \"Critique_of_Practical_Reason\", \"https://www.gutenberg.org/cache/epub/5683/pg5683.txt\"),\n",
        "    (\"Kant\", \"Fundamental_Principles_of_the_Metaphysic_of_Morals\", \"https://www.gutenberg.org/cache/epub/5682/pg5682.txt\"),\n",
        "    (\"Kant\", \"Kant's Prolegomena to Any Future Metaphysics\", \"https://www.gutenberg.org/cache/epub/52821/pg52821.txt\"),\n",
        "    (\"Kant\", \"Kant's Critique of Judgement\", \"https://www.gutenberg.org/cache/epub/48433/pg48433.txt\"),\n",
        "    (\"Kant\", \"The Metaphysical Elements of Ethics\", \"https://www.gutenberg.org/cache/epub/5684/pg5684.txt\")\n",
        "]\n",
        "\n",
        "def download(url, outpath):\n",
        "    r = requests.get(url, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    outpath.write_bytes(r.content)\n",
        "\n",
        "rows = []\n",
        "\n",
        "for author, work, url in BOOK_URLS:\n",
        "    outpath = OUT_DIR / f\"{author}__{work}.txt\"\n",
        "\n",
        "    if not outpath.exists():\n",
        "        print(f\"Downloading {work}...\")\n",
        "        download(url, outpath)\n",
        "    else:\n",
        "        print(f\"Already downloaded {work}\")\n",
        "\n",
        "    # Robust decoding with reporting\n",
        "    text = None\n",
        "    for enc in [\"utf-8\", \"utf-8-sig\", \"ISO-8859-1\", \"cp1252\"]:\n",
        "        try:\n",
        "            text = outpath.read_text(encoding=enc)\n",
        "            print(f\"  ✓ Decoded with {enc}\")\n",
        "            break\n",
        "        except UnicodeDecodeError:\n",
        "            continue\n",
        "\n",
        "    if text is None:\n",
        "        raise ValueError(f\"Could not decode {outpath} with any encoding\")\n",
        "\n",
        "    rows.append({\n",
        "        \"author_label\": author,\n",
        "        \"work\": work,\n",
        "        \"source_url\": url,\n",
        "        \"text\": text\n",
        "    })\n",
        "\n",
        "df_books = pd.DataFrame(rows)\n",
        "\n",
        "print(\"Books loaded:\", len(df_books))\n",
        "display(df_books[[\"author_label\", \"work\", \"source_url\"]])\n",
        "print(df_books[\"author_label\"].value_counts())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:15:28.570192Z",
          "iopub.execute_input": "2026-01-02T14:15:28.570473Z",
          "iopub.status.idle": "2026-01-02T14:15:32.687202Z",
          "shell.execute_reply.started": "2026-01-02T14:15:28.570451Z",
          "shell.execute_reply": "2026-01-02T14:15:32.686415Z"
        },
        "id": "suBTngcrdPS8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Strip Gutenberg boilderplate"
      ],
      "metadata": {
        "id": "cwfLor6ddPS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "START_RE = re.compile(r\"\\*\\*\\*\\s*START OF (THE )?PROJECT GUTENBERG EBOOK.*?\\*\\*\\*\", re.IGNORECASE)\n",
        "END_RE   = re.compile(r\"\\*\\*\\*\\s*END OF (THE )?PROJECT GUTENBERG EBOOK.*?\\*\\*\\*\", re.IGNORECASE)\n",
        "\n",
        "def strip_gutenberg_boilerplate(text: str) -> str:\n",
        "    start = START_RE.search(text)\n",
        "    end = END_RE.search(text)\n",
        "    if start and end and start.end() < end.start():\n",
        "        return text[start.end():end.start()].strip()\n",
        "    return text.strip()\n",
        "\n",
        "df_books[\"text\"] = df_books[\"text\"].apply(strip_gutenberg_boilerplate)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:15:39.399164Z",
          "iopub.execute_input": "2026-01-02T14:15:39.399503Z",
          "iopub.status.idle": "2026-01-02T14:15:39.424986Z",
          "shell.execute_reply.started": "2026-01-02T14:15:39.399481Z",
          "shell.execute_reply": "2026-01-02T14:15:39.424044Z"
        },
        "id": "QKkCdOIZdPS8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_CHECKPOINT = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT, use_fast=True)\n",
        "CHUNK_SIZE = 125\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "label2id = {\"Kant\": 0, \"Nietzsche\": 1}\n",
        "id2label = {0: \"Kant\", 1: \"Nietzsche\"}\n",
        "\n",
        "print(\"SETUP\")\n",
        "print(f\"Model: {MODEL_CHECKPOINT}\")\n",
        "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
        "print(f\"Chunk size: {CHUNK_SIZE} tokens\")\n",
        "print(f\"Random state: {RANDOM_STATE}\")\n",
        "print(f\"Label mapping: {label2id}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:15:41.510951Z",
          "iopub.execute_input": "2026-01-02T14:15:41.511255Z",
          "iopub.status.idle": "2026-01-02T14:15:42.723908Z",
          "shell.execute_reply.started": "2026-01-02T14:15:41.511233Z",
          "shell.execute_reply": "2026-01-02T14:15:42.723233Z"
        },
        "id": "HIPyyPW-dPS9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chunking functions\n",
        "\n",
        "This creates text chunks and stores metadata for leakage-safe splitting"
      ],
      "metadata": {
        "id": "ZezafdLMdPS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_book_to_token_ids(text, tokenizer, chunk_size=125, min_chunk_tokens=80):\n",
        "    # Tokenize full book into IDs (can be very long — that's okay)\n",
        "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    # Slice into chunks\n",
        "    chunks = []\n",
        "    for i in range(0, len(ids), chunk_size):\n",
        "        chunk_ids = ids[i:i+chunk_size]\n",
        "        if len(chunk_ids) < min_chunk_tokens:\n",
        "            continue\n",
        "        chunks.append(chunk_ids)\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:15:44.843859Z",
          "iopub.execute_input": "2026-01-02T14:15:44.844532Z",
          "iopub.status.idle": "2026-01-02T14:15:44.849076Z",
          "shell.execute_reply.started": "2026-01-02T14:15:44.844508Z",
          "shell.execute_reply": "2026-01-02T14:15:44.848260Z"
        },
        "id": "AhZJOXkkdPS9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "for _, r in df_books.iterrows():\n",
        "    author = r[\"author_label\"]\n",
        "    work = r[\"work\"]\n",
        "    text = r[\"text\"]\n",
        "\n",
        "    chunk_ids_list = chunk_book_to_token_ids(\n",
        "        text=text,\n",
        "        tokenizer=tokenizer,\n",
        "        chunk_size=CHUNK_SIZE,\n",
        "        min_chunk_tokens=80\n",
        "    )\n",
        "\n",
        "    for j, chunk_ids in enumerate(chunk_ids_list):\n",
        "        rows.append({\n",
        "            \"author_label\": author,\n",
        "            \"work\": work,\n",
        "            \"chunk_id\": j,\n",
        "            \"input_ids\": chunk_ids\n",
        "        })\n",
        "\n",
        "df_chunks = pd.DataFrame(rows)\n",
        "print(\"Total chunks:\", len(df_chunks))\n",
        "display(df_chunks.head())\n",
        "print(df_chunks[\"author_label\"].value_counts())\n",
        "print(df_chunks[\"work\"].value_counts())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:15:47.266081Z",
          "iopub.execute_input": "2026-01-02T14:15:47.266377Z",
          "iopub.status.idle": "2026-01-02T14:15:51.416011Z",
          "shell.execute_reply.started": "2026-01-02T14:15:47.266354Z",
          "shell.execute_reply": "2026-01-02T14:15:51.415250Z"
        },
        "id": "I7WeXvKUdPS9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify chunk length distribution\n",
        "chunk_lengths = [len(chunk_ids) for chunk_ids in df_chunks['input_ids']]\n",
        "\n",
        "print(\"CHUNK LENGTH VERIFICATION\")\n",
        "print(f\"Target size: {CHUNK_SIZE} tokens\")\n",
        "print(f\"Min allowed: 80 tokens\")\n",
        "print(f\"\\nActual distribution:\")\n",
        "print(f\"  Mean: {np.mean(chunk_lengths):.1f}\")\n",
        "print(f\"  Min: {np.min(chunk_lengths)}\")\n",
        "print(f\"  Max: {np.max(chunk_lengths)}\")\n",
        "print(f\"  Chunks == {CHUNK_SIZE}: {sum(1 for l in chunk_lengths if l == CHUNK_SIZE)} ({sum(1 for l in chunk_lengths if l == CHUNK_SIZE)/len(chunk_lengths)*100:.1f}%)\")\n",
        "print(f\"  Chunks < {CHUNK_SIZE}: {sum(1 for l in chunk_lengths if l < CHUNK_SIZE)} ({sum(1 for l in chunk_lengths if l < CHUNK_SIZE)/len(chunk_lengths)*100:.1f}%)\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:15:53.426112Z",
          "iopub.execute_input": "2026-01-02T14:15:53.426836Z",
          "iopub.status.idle": "2026-01-02T14:15:53.437306Z",
          "shell.execute_reply.started": "2026-01-02T14:15:53.426808Z",
          "shell.execute_reply": "2026-01-02T14:15:53.436457Z"
        },
        "id": "x61e_RYMdPS9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train test split"
      ],
      "metadata": {
        "id": "RR2dN9OYdPS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_train_val_test_stratified(\n",
        "    df_chunks: pd.DataFrame,\n",
        "    test_works_per_author: int = 1,\n",
        "    val_pct: float = 0.15,\n",
        "    random_state: int = 42\n",
        "):\n",
        "    \"\"\"\n",
        "    Stratified split that:\n",
        "    1. Holds out N works per author for test (centered around median)\n",
        "    2. Uses val_pct% of remaining chunks for validation\n",
        "    3. Uses remaining chunks for training\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(random_state)\n",
        "\n",
        "    # Count chunks per work\n",
        "    counts = (\n",
        "        df_chunks\n",
        "        .groupby([\"author_label\", \"work\"])\n",
        "        .size()\n",
        "        .reset_index(name=\"n_chunks\")\n",
        "    )\n",
        "\n",
        "    held_out = {}\n",
        "    test_parts = []\n",
        "    pool_parts = []\n",
        "\n",
        "    # Hold out test works per author\n",
        "    for author in sorted(df_chunks[\"author_label\"].unique()):\n",
        "        sub = counts[counts[\"author_label\"] == author].sort_values(\"n_chunks\")\n",
        "\n",
        "        if len(sub) < test_works_per_author + 1:\n",
        "            raise ValueError(\n",
        "                f\"Need at least {test_works_per_author + 1} works for {author}, \"\n",
        "                f\"but only have {len(sub)}\"\n",
        "            )\n",
        "\n",
        "        # Pick works centered around median\n",
        "        n_works = len(sub)\n",
        "        median_idx = n_works // 2\n",
        "\n",
        "        if test_works_per_author == 1:\n",
        "            # Single median work\n",
        "            test_indices = [median_idx]\n",
        "        else:\n",
        "            # Multiple works centered around median\n",
        "            half = test_works_per_author // 2\n",
        "            start_idx = max(0, median_idx - half)\n",
        "            end_idx = min(n_works, start_idx + test_works_per_author)\n",
        "            test_indices = list(range(start_idx, end_idx))\n",
        "\n",
        "        test_works = sub.iloc[test_indices][\"work\"].tolist()\n",
        "\n",
        "        print(f\"\\n{author} - Test works ({len(test_works)}):\")\n",
        "        for tw in test_works:\n",
        "            n_chunks = sub[sub[\"work\"] == tw][\"n_chunks\"].iloc[0]\n",
        "            print(f\"  - {tw} ({n_chunks} chunks)\")\n",
        "\n",
        "        held_out[author] = test_works[0] if len(test_works) == 1 else test_works\n",
        "\n",
        "        # Create masks\n",
        "        test_mask = (df_chunks[\"author_label\"] == author) & (df_chunks[\"work\"].isin(test_works))\n",
        "        pool_mask = (df_chunks[\"author_label\"] == author) & (~df_chunks[\"work\"].isin(test_works))\n",
        "\n",
        "        test_parts.append(df_chunks[test_mask])\n",
        "        pool_parts.append(df_chunks[pool_mask])\n",
        "\n",
        "    test_df = pd.concat(test_parts, ignore_index=True)\n",
        "    pool_df = pd.concat(pool_parts, ignore_index=True)\n",
        "\n",
        "    # Verify no leakage\n",
        "    test_works_all = set(test_df[\"work\"].unique())\n",
        "    pool_works_all = set(pool_df[\"work\"].unique())\n",
        "    overlap = test_works_all & pool_works_all\n",
        "    assert len(overlap) == 0, f\"Leakage: {overlap}\"\n",
        "\n",
        "    print(f\"\\nPool available for train/val:\")\n",
        "    print(f\"  Kant: {len(pool_df[pool_df['author_label'] == 'Kant'])} chunks\")\n",
        "    print(f\"  Nietzsche: {len(pool_df[pool_df['author_label'] == 'Nietzsche'])} chunks\")\n",
        "\n",
        "    # Split pool into train/val PER AUTHOR (maintain balance)\n",
        "    train_parts = []\n",
        "    val_parts = []\n",
        "\n",
        "    for author in sorted(pool_df[\"author_label\"].unique()):\n",
        "        author_pool = pool_df[pool_df[\"author_label\"] == author].copy()\n",
        "\n",
        "        n_val = int(len(author_pool) * val_pct)\n",
        "        n_train = len(author_pool) - n_val\n",
        "\n",
        "        # Shuffle and split\n",
        "        shuffled = author_pool.sample(frac=1, random_state=random_state)\n",
        "\n",
        "        train_parts.append(shuffled.iloc[:n_train])\n",
        "        val_parts.append(shuffled.iloc[n_train:])\n",
        "\n",
        "        print(f\"\\n{author} split:\")\n",
        "        print(f\"  Train: {n_train} chunks ({n_train/len(author_pool)*100:.1f}%)\")\n",
        "        print(f\"  Val: {n_val} chunks ({n_val/len(author_pool)*100:.1f}%)\")\n",
        "\n",
        "    train_df = pd.concat(train_parts, ignore_index=True).sample(frac=1, random_state=random_state)\n",
        "    val_df = pd.concat(val_parts, ignore_index=True).sample(frac=1, random_state=random_state)\n",
        "\n",
        "    return train_df, val_df, test_df, held_out"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:15:55.848288Z",
          "iopub.execute_input": "2026-01-02T14:15:55.848564Z",
          "iopub.status.idle": "2026-01-02T14:15:55.859860Z",
          "shell.execute_reply.started": "2026-01-02T14:15:55.848545Z",
          "shell.execute_reply": "2026-01-02T14:15:55.859146Z"
        },
        "id": "L36kREYZdPS9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STRATIFIED TRAIN/VAL/TEST SPLIT\")\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "train_df, val_df, test_df, held_out = split_train_val_test_stratified(\n",
        "    df_chunks,\n",
        "    test_works_per_author=1,\n",
        "    val_pct=0.15,  # 15% of remaining for validation\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(\"SPLIT SUMMARY\")\n",
        "print(f\"\\nTRAIN: {len(train_df)} chunks\")\n",
        "print(train_df[\"author_label\"].value_counts())\n",
        "print(\"\\nTrain works:\")\n",
        "print(train_df.groupby(['author_label', 'work']).size())\n",
        "\n",
        "print(f\"\\nVAL: {len(val_df)} chunks\")\n",
        "print(val_df[\"author_label\"].value_counts())\n",
        "print(\"\\nVal works:\")\n",
        "print(val_df.groupby(['author_label', 'work']).size())\n",
        "\n",
        "print(f\"\\nTEST: {len(test_df)} chunks\")\n",
        "print(test_df[\"author_label\"].value_counts())\n",
        "print(\"\\nTest works:\")\n",
        "print(test_df.groupby(['author_label', 'work']).size())\n",
        "\n",
        "# Final percentages\n",
        "total = len(df_chunks)\n",
        "print(\"OVERALL DATA USAGE\")\n",
        "print(f\"Total chunks: {total}\")\n",
        "print(f\"  Train: {len(train_df)} ({len(train_df)/total*100:.1f}%)\")\n",
        "print(f\"  Val: {len(val_df)} ({len(val_df)/total*100:.1f}%)\")\n",
        "print(f\"  Test: {len(test_df)} ({len(test_df)/total*100:.1f}%)\")\n",
        "print(f\"  Used: {len(train_df)+len(val_df)+len(test_df)} ({(len(train_df)+len(val_df)+len(test_df))/total*100:.1f}%)\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:15:59.628964Z",
          "iopub.execute_input": "2026-01-02T14:15:59.629729Z",
          "iopub.status.idle": "2026-01-02T14:15:59.671732Z",
          "shell.execute_reply.started": "2026-01-02T14:15:59.629705Z",
          "shell.execute_reply": "2026-01-02T14:15:59.671125Z"
        },
        "id": "76I39QU9dPS-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CROSS-WORK GENERALIZATION CHECK\")\n",
        "\n",
        "# Ensure test works are completely unseen\n",
        "train_works = set(train_df['work'].unique())\n",
        "val_works = set(val_df['work'].unique())\n",
        "test_works = set(test_df['work'].unique())\n",
        "\n",
        "print(f\"\\nTrain works: {len(train_works)}\")\n",
        "print(f\"Val works: {len(val_works)}\")\n",
        "print(f\"Test works: {len(test_works)}\")\n",
        "\n",
        "overlap_train_test = train_works & test_works\n",
        "overlap_val_test = val_works & test_works\n",
        "\n",
        "assert len(overlap_train_test) == 0, f\"❌ LEAKAGE: {overlap_train_test}\"\n",
        "assert len(overlap_val_test) == 0, f\"❌ LEAKAGE: {overlap_val_test}\"\n",
        "\n",
        "print(\"✓ No work appears in both train and test\")\n",
        "print(\"✓ No work appears in both val and test\")\n",
        "print(f\"\\nTest set represents {len(test_works)}/{len(df_chunks['work'].unique())} works per author\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:16:08.520431Z",
          "iopub.execute_input": "2026-01-02T14:16:08.521268Z",
          "iopub.status.idle": "2026-01-02T14:16:08.532914Z",
          "shell.execute_reply.started": "2026-01-02T14:16:08.521232Z",
          "shell.execute_reply": "2026-01-02T14:16:08.531977Z"
        },
        "id": "j5SnM3nVdPS-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df[\"labels\"] = df[\"author_label\"].map(label2id).astype(int)\n",
        "\n",
        "    if df[\"labels\"].isna().any():\n",
        "        unmapped = df[df[\"labels\"].isna()][\"author_label\"].unique()\n",
        "        raise ValueError(f\"Unmapped authors found: {unmapped}\")\n",
        "\n",
        "    # keep only what Trainer needs (+ optional metadata if you want)\n",
        "    return df[[\"input_ids\", \"labels\", \"author_label\", \"work\", \"chunk_id\"]]\n",
        "\n",
        "train_ready = prepare_df(train_df)\n",
        "val_ready   = prepare_df(val_df)\n",
        "test_ready  = prepare_df(test_df)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:16:10.923472Z",
          "iopub.execute_input": "2026-01-02T14:16:10.924298Z",
          "iopub.status.idle": "2026-01-02T14:16:10.938393Z",
          "shell.execute_reply.started": "2026-01-02T14:16:10.924263Z",
          "shell.execute_reply": "2026-01-02T14:16:10.937471Z"
        },
        "id": "GAEGicbidPS-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = Dataset.from_pandas(train_ready, preserve_index=False)\n",
        "ds_val   = Dataset.from_pandas(val_ready, preserve_index=False)\n",
        "ds_test  = Dataset.from_pandas(test_ready, preserve_index=False)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:16:13.476053Z",
          "iopub.execute_input": "2026-01-02T14:16:13.476338Z",
          "iopub.status.idle": "2026-01-02T14:16:13.652745Z",
          "shell.execute_reply.started": "2026-01-02T14:16:13.476317Z",
          "shell.execute_reply": "2026-01-02T14:16:13.652018Z"
        },
        "id": "k0nx0TuDdPS-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluate.load(\"accuracy\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    out = {}\n",
        "    out.update(accuracy.compute(predictions=preds, references=labels))\n",
        "    out[\"f1_macro\"] = f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
        "    out[\"f1_weighted\"] = f1.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"]\n",
        "    return out"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:16:24.473360Z",
          "iopub.execute_input": "2026-01-02T14:16:24.473720Z",
          "iopub.status.idle": "2026-01-02T14:16:25.446317Z",
          "shell.execute_reply.started": "2026-01-02T14:16:24.473693Z",
          "shell.execute_reply": "2026-01-02T14:16:25.445568Z"
        },
        "id": "XvLRXxsNdPS-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_CHECKPOINT,\n",
        "    num_labels=2,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"distilbert_kant_vs_nietzsche\",\n",
        "\n",
        "    # Evaluation & Saving - OPTIMIZED\n",
        "    eval_strategy=\"epoch\",              # Keep - needed for early stopping\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,                 # Only keep 2 best checkpoints (save space)\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    greater_is_better=True,\n",
        "\n",
        "    # Training Schedule\n",
        "    num_train_epochs=5,\n",
        "    warmup_ratio=0.1,\n",
        "    learning_rate=2e-5,\n",
        "\n",
        "    # Batch Sizes - OPTIMIZED FOR SPEED\n",
        "    per_device_train_batch_size=32,     # INCREASED from 16 (2x faster if memory allows)\n",
        "    per_device_eval_batch_size=64,      # INCREASED from 32 (eval can use more memory)\n",
        "    gradient_accumulation_steps=1,      # Keep at 1 for speed\n",
        "\n",
        "    # Optimization\n",
        "    weight_decay=0.01,\n",
        "    adam_epsilon=1e-8,\n",
        "    max_grad_norm=1.0,\n",
        "\n",
        "    # Performance - OPTIMIZED\n",
        "    fp16=True,                          # FORCE enable (don't just check availability)\n",
        "    fp16_opt_level=\"O1\",                # Mixed precision optimization level\n",
        "    dataloader_num_workers=2,           # ADDED: Parallel data loading (faster)\n",
        "    dataloader_pin_memory=True,         # ADDED: Faster CPU-GPU transfer\n",
        "\n",
        "    # Logging - REDUCED OVERHEAD\n",
        "    logging_steps=100,                  # INCREASED from 50 (less frequent = faster)\n",
        "    logging_first_step=True,\n",
        "    report_to=\"none\",\n",
        "\n",
        "    # Misc\n",
        "    disable_tqdm=False,                 # Keep progress bar\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=ds_train,\n",
        "    eval_dataset=ds_val,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
        ")\n",
        "\n",
        "# Check configuration before training\n",
        "print(\"=\" * 80)\n",
        "print(\"TRAINING CONFIGURATION\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Device: {training_args.device}\")\n",
        "print(f\"FP16: {training_args.fp16}\")\n",
        "print(f\"Train batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"Eval batch size: {training_args.per_device_eval_batch_size}\")\n",
        "print(f\"Data workers: {training_args.dataloader_num_workers}\")\n",
        "print(f\"Total training samples: {len(ds_train)}\")\n",
        "print(f\"Steps per epoch: {len(ds_train) // training_args.per_device_train_batch_size}\")\n",
        "print(f\"Total steps: ~{(len(ds_train) // training_args.per_device_train_batch_size) * training_args.num_train_epochs}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Train\n",
        "trainer.train()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:16:28.070830Z",
          "iopub.execute_input": "2026-01-02T14:16:28.071503Z",
          "iopub.status.idle": "2026-01-02T14:21:14.954635Z",
          "shell.execute_reply.started": "2026-01-02T14:16:28.071476Z",
          "shell.execute_reply": "2026-01-02T14:21:14.953864Z"
        },
        "id": "yzBISyo0dPS-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure we are using the best checkpoint (Trainer already loads it if load_best_model_at_end=True)\n",
        "best_model = trainer.model\n",
        "best_model.eval()  # \"freeze\" for inference (no dropout, no gradients)\n",
        "\n",
        "SAVE_DIR = \"best_distilbert_author_clf\"\n",
        "trainer.save_model(SAVE_DIR)\n",
        "tokenizer.save_pretrained(SAVE_DIR)\n",
        "\n",
        "print(\"Saved best model to:\", SAVE_DIR)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:26:18.713944Z",
          "iopub.execute_input": "2026-01-02T14:26:18.714598Z",
          "iopub.status.idle": "2026-01-02T14:26:19.281121Z",
          "shell.execute_reply.started": "2026-01-02T14:26:18.714569Z",
          "shell.execute_reply": "2026-01-02T14:26:19.280353Z"
        },
        "id": "DYm9wD6edPS-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "wuIqSqK4dPS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full test evaluation (no training happens here)\n",
        "test_metrics = trainer.evaluate(ds_test)\n",
        "print(\"Full test metrics:\", test_metrics)\n",
        "\n",
        "# Get predicted probabilities + predicted labels\n",
        "pred_out = trainer.predict(ds_test)\n",
        "logits = pred_out.predictions\n",
        "y_true = pred_out.label_ids\n",
        "y_pred = np.argmax(logits, axis=-1)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:26:21.501102Z",
          "iopub.execute_input": "2026-01-02T14:26:21.501665Z",
          "iopub.status.idle": "2026-01-02T14:26:27.267967Z",
          "shell.execute_reply.started": "2026-01-02T14:26:21.501614Z",
          "shell.execute_reply": "2026-01-02T14:26:27.267362Z"
        },
        "id": "wojypurndPS-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "print(\"Confusion matrix (rows=true, cols=pred):\")\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(cm)\n",
        "\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(\n",
        "    y_true, y_pred,\n",
        "    target_names=[id2label[0], id2label[1]],\n",
        "    digits=3\n",
        "))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:26:28.444990Z",
          "iopub.execute_input": "2026-01-02T14:26:28.445494Z",
          "iopub.status.idle": "2026-01-02T14:26:28.460569Z",
          "shell.execute_reply.started": "2026-01-02T14:26:28.445469Z",
          "shell.execute_reply": "2026-01-02T14:26:28.459919Z"
        },
        "id": "Ct77P8WsdPS_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert test dataset back to a pandas df (it already has author_label/work/chunk_id)\n",
        "test_pred_df = test_ready.copy()\n",
        "\n",
        "test_pred_df[\"y_true\"] = y_true\n",
        "test_pred_df[\"y_pred\"] = y_pred\n",
        "test_pred_df[\"pred_label\"] = test_pred_df[\"y_pred\"].map(id2label)\n",
        "\n",
        "# Optional: add model confidence via softmax\n",
        "def softmax(x):\n",
        "    x = x - np.max(x, axis=-1, keepdims=True)\n",
        "    ex = np.exp(x)\n",
        "    return ex / np.sum(ex, axis=-1, keepdims=True)\n",
        "\n",
        "probs = softmax(logits)\n",
        "test_pred_df[\"p_kant\"] = probs[:, label2id[\"Kant\"]]\n",
        "test_pred_df[\"p_nietzsche\"] = probs[:, label2id[\"Nietzsche\"]]\n",
        "test_pred_df[\"correct\"] = (test_pred_df[\"y_true\"] == test_pred_df[\"y_pred\"])\n",
        "\n",
        "print(\"Test accuracy (manual):\", test_pred_df[\"correct\"].mean())\n",
        "display(test_pred_df.head())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:26:31.243687Z",
          "iopub.execute_input": "2026-01-02T14:26:31.244268Z",
          "iopub.status.idle": "2026-01-02T14:26:31.270526Z",
          "shell.execute_reply.started": "2026-01-02T14:26:31.244243Z",
          "shell.execute_reply": "2026-01-02T14:26:31.269930Z"
        },
        "id": "eWwlFPvLdPS_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ERROR ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ERROR ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Overall error rate\n",
        "wrong = test_pred_df[~test_pred_df[\"correct\"]].copy()\n",
        "error_rate = len(wrong) / len(test_pred_df)\n",
        "\n",
        "print(f\"\\nTotal errors: {len(wrong)}/{len(test_pred_df)} ({error_rate*100:.2f}%)\")\n",
        "\n",
        "# Error distribution by author\n",
        "print(f\"\\nErrors by true author:\")\n",
        "for author in ['Kant', 'Nietzsche']:\n",
        "    author_errors = wrong[wrong['author_label'] == author]\n",
        "    author_total = len(test_pred_df[test_pred_df['author_label'] == author])\n",
        "\n",
        "    if author_total > 0:\n",
        "        author_error_rate = len(author_errors) / author_total\n",
        "        print(f\"  {author}: {len(author_errors)}/{author_total} ({author_error_rate*100:.2f}%)\")\n",
        "\n",
        "# Confidence distribution for errors\n",
        "wrong[\"conf_pred\"] = np.where(\n",
        "    wrong[\"pred_label\"] == \"Kant\",\n",
        "    wrong[\"p_kant\"],\n",
        "    wrong[\"p_nietzsche\"]\n",
        ")\n",
        "\n",
        "print(f\"\\nError confidence distribution:\")\n",
        "print(f\"  Mean confidence: {wrong['conf_pred'].mean():.3f}\")\n",
        "print(f\"  Median confidence: {wrong['conf_pred'].median():.3f}\")\n",
        "print(f\"  High confidence errors (>0.7): {(wrong['conf_pred'] > 0.7).sum()}\")\n",
        "\n",
        "# Top 10 most confident errors\n",
        "print(f\"\\nTop 10 most confident errors:\")\n",
        "print(\"-\" * 80)\n",
        "top_errors = wrong.sort_values(\"conf_pred\", ascending=False).head(10)\n",
        "\n",
        "for idx, row in top_errors.iterrows():\n",
        "    print(f\"\\nTrue: {row['author_label']} → Predicted: {row['pred_label']} (conf: {row['conf_pred']:.3f})\")\n",
        "    print(f\"  Work: {row['work'][:50]}...\")\n",
        "    print(f\"  Chunk ID: {row['chunk_id']}\")\n",
        "\n",
        "    # Optionally decode and show snippet\n",
        "    if 'input_ids' in row and isinstance(row['input_ids'], list):\n",
        "        snippet = tokenizer.decode(row['input_ids'][:50])\n",
        "        print(f\"  Text: {snippet}...\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:26:33.142184Z",
          "iopub.execute_input": "2026-01-02T14:26:33.142470Z",
          "iopub.status.idle": "2026-01-02T14:26:33.160428Z",
          "shell.execute_reply.started": "2026-01-02T14:26:33.142451Z",
          "shell.execute_reply": "2026-01-02T14:26:33.159653Z"
        },
        "id": "KB9lstHcdPS_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# H1 EVALUATION: Cross-Work Generalization Performance\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"H1: FINE-TUNED TRANSFORMER ACCURACY\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Hypothesis: Achieve >80% accuracy on completely unseen works\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Test set characteristics\n",
        "print(f\"\\nTest Set Composition:\")\n",
        "print(f\"  Total samples: {len(test_pred_df)}\")\n",
        "print(f\"  Held-out works: {list(held_out.values())}\")\n",
        "print(f\"  Kant: {len(test_pred_df[test_pred_df['author_label'] == 'Kant'])} samples\")\n",
        "print(f\"  Nietzsche: {len(test_pred_df[test_pred_df['author_label'] == 'Nietzsche'])} samples\")\n",
        "\n",
        "# Overall test accuracy\n",
        "test_accuracy = test_metrics['eval_accuracy']\n",
        "test_f1 = test_metrics['eval_f1_macro']\n",
        "\n",
        "print(f\"\\n\" + \"-\" * 80)\n",
        "print(\"CROSS-WORK TEST PERFORMANCE:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"  Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "print(f\"  F1 Macro: {test_f1:.4f}\")\n",
        "print(f\"  F1 Weighted: {test_metrics['eval_f1_weighted']:.4f}\")\n",
        "\n",
        "# H1 verdict\n",
        "threshold = 0.80\n",
        "if test_accuracy >= threshold:\n",
        "    print(f\"\\n✓ H1 STRONGLY SUPPORTED\")\n",
        "    print(f\"  Model achieves {test_accuracy*100:.2f}% accuracy (>{threshold*100:.0f}% threshold)\")\n",
        "    print(f\"  Demonstrates robust cross-work style learning\")\n",
        "elif test_accuracy >= 0.70:\n",
        "    print(f\"\\n⚠ H1 PARTIALLY SUPPORTED\")\n",
        "    print(f\"  Model achieves {test_accuracy*100:.2f}% accuracy\")\n",
        "    print(f\"  Good performance but below {threshold*100:.0f}% threshold\")\n",
        "else:\n",
        "    print(f\"\\n✗ H1 NOT SUPPORTED\")\n",
        "    print(f\"  Model only achieves {test_accuracy*100:.2f}% accuracy\")\n",
        "\n",
        "# Per-work breakdown\n",
        "print(f\"\\n\" + \"-\" * 80)\n",
        "print(\"PER-WORK PERFORMANCE:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for work in held_out.values():\n",
        "    work_df = test_pred_df[test_pred_df['work'] == work]\n",
        "    work_acc = work_df['correct'].mean()\n",
        "    work_author = work_df['author_label'].iloc[0]\n",
        "\n",
        "    print(f\"\\n{work_author}: '{work}'\")\n",
        "    print(f\"  Samples: {len(work_df)}\")\n",
        "    print(f\"  Accuracy: {work_acc:.4f} ({work_acc*100:.2f}%)\")\n",
        "    print(f\"  Correct: {work_df['correct'].sum()}/{len(work_df)}\")\n",
        "\n",
        "# Class-specific performance\n",
        "print(f\"\\n\" + \"-\" * 80)\n",
        "print(\"PER-CLASS PERFORMANCE:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for author_name, author_id in label2id.items():\n",
        "    author_mask = test_pred_df['author_label'] == author_name\n",
        "    author_acc = test_pred_df[author_mask]['correct'].mean()\n",
        "    n_samples = author_mask.sum()\n",
        "\n",
        "    print(f\"\\n{author_name}:\")\n",
        "    print(f\"  Samples: {n_samples}\")\n",
        "    print(f\"  Accuracy: {author_acc:.4f} ({author_acc*100:.2f}%)\")\n",
        "    print(f\"  Correct: {test_pred_df[author_mask]['correct'].sum()}/{n_samples}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:26:37.831196Z",
          "iopub.execute_input": "2026-01-02T14:26:37.831924Z",
          "iopub.status.idle": "2026-01-02T14:26:37.850683Z",
          "shell.execute_reply.started": "2026-01-02T14:26:37.831900Z",
          "shell.execute_reply": "2026-01-02T14:26:37.849777Z"
        },
        "id": "nSj4G3XedPS_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Kant', 'Nietzsche'],\n",
        "            yticklabels=['Kant', 'Nietzsche'],\n",
        "            ax=axes[0, 0])\n",
        "axes[0, 0].set_title(f'Confusion Matrix\\n(Test Accuracy: {test_accuracy:.1%})')\n",
        "axes[0, 0].set_ylabel('True Label')\n",
        "axes[0, 0].set_xlabel('Predicted Label')\n",
        "\n",
        "# 2. Prediction Confidence Distribution\n",
        "correct_conf = test_pred_df[test_pred_df['correct']].apply(\n",
        "    lambda row: max(row['p_kant'], row['p_nietzsche']), axis=1\n",
        ")\n",
        "incorrect_conf = test_pred_df[~test_pred_df['correct']].apply(\n",
        "    lambda row: max(row['p_kant'], row['p_nietzsche']), axis=1\n",
        ")\n",
        "\n",
        "axes[0, 1].hist([correct_conf, incorrect_conf],\n",
        "                bins=30, alpha=0.7, label=['Correct', 'Incorrect'])\n",
        "axes[0, 1].axvline(x=0.8, color='red', linestyle='--', label='High confidence (>0.8)')\n",
        "axes[0, 1].set_xlabel('Prediction Confidence')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_title('Confidence Distribution')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# 3. Per-Work Accuracy\n",
        "work_acc = []\n",
        "work_names = []\n",
        "for work in held_out.values():\n",
        "    work_df = test_pred_df[test_pred_df['work'] == work]\n",
        "    work_acc.append(work_df['correct'].mean())\n",
        "    work_names.append(work[:40])  # Truncate long names\n",
        "\n",
        "axes[1, 0].barh(range(len(work_names)), work_acc, alpha=0.7)\n",
        "axes[1, 0].set_yticks(range(len(work_names)))\n",
        "axes[1, 0].set_yticklabels(work_names, fontsize=9)\n",
        "axes[1, 0].set_xlabel('Accuracy')\n",
        "axes[1, 0].set_title('Accuracy by Test Work')\n",
        "axes[1, 0].axvline(x=0.8, color='red', linestyle='--', label='80% threshold')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].set_xlim([0, 1])\n",
        "\n",
        "# 4. Training History (FIXED)\n",
        "if hasattr(trainer.state, 'log_history'):\n",
        "    log_history = trainer.state.log_history\n",
        "\n",
        "    # Extract training and validation metrics separately\n",
        "    train_steps = []\n",
        "    train_losses = []\n",
        "    val_steps = []\n",
        "    val_losses = []\n",
        "\n",
        "    for entry in log_history:\n",
        "        if 'loss' in entry and 'epoch' in entry:\n",
        "            train_steps.append(entry.get('step', len(train_steps)))\n",
        "            train_losses.append(entry['loss'])\n",
        "        if 'eval_loss' in entry and 'epoch' in entry:\n",
        "            val_steps.append(entry.get('step', len(val_steps)))\n",
        "            val_losses.append(entry['eval_loss'])\n",
        "\n",
        "    if train_losses and val_losses:\n",
        "        axes[1, 1].plot(train_steps, train_losses, label='Train Loss', marker='o', alpha=0.7)\n",
        "        axes[1, 1].plot(val_steps, val_losses, label='Val Loss', marker='s', markersize=8)\n",
        "        axes[1, 1].set_xlabel('Training Steps')\n",
        "        axes[1, 1].set_ylabel('Loss')\n",
        "        axes[1, 1].set_title('Training & Validation Loss')\n",
        "        axes[1, 1].legend()\n",
        "        axes[1, 1].grid(alpha=0.3)\n",
        "    else:\n",
        "        axes[1, 1].text(0.5, 0.5, 'Training history\\nnot available',\n",
        "                       ha='center', va='center', fontsize=12)\n",
        "        axes[1, 1].axis('off')\n",
        "else:\n",
        "    axes[1, 1].text(0.5, 0.5, 'Training history\\nnot available',\n",
        "                   ha='center', va='center', fontsize=12)\n",
        "    axes[1, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('h1_evaluation.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Visualization saved as 'h1_evaluation.png'\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:26:42.500108Z",
          "iopub.execute_input": "2026-01-02T14:26:42.500836Z",
          "iopub.status.idle": "2026-01-02T14:26:44.746134Z",
          "shell.execute_reply.started": "2026-01-02T14:26:42.500810Z",
          "shell.execute_reply": "2026-01-02T14:26:44.745361Z"
        },
        "id": "sigI7_m6dPS_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Topic modeling with BERTopic"
      ],
      "metadata": {
        "id": "TbukvgL-dPS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# BERTOPIC ANALYSIS: Topic Distribution Exploration\n",
        "# ============================================================================\n",
        "# Purpose: Analyze topic overlap between authors to assess feasibility of\n",
        "#          topic-based control for H2\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"BERTOPIC: TOPIC DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import hdbscan\n",
        "\n",
        "# ============================================================================\n",
        "# Step 1: Prepare Data\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nStep 1: Preparing text data...\")\n",
        "\n",
        "# Decode all datasets\n",
        "def decode_dataset(ds):\n",
        "    return [tokenizer.decode(ids, skip_special_tokens=True) for ids in ds[\"input_ids\"]]\n",
        "\n",
        "train_texts = decode_dataset(ds_train)\n",
        "val_texts = decode_dataset(ds_val)\n",
        "test_texts = decode_dataset(ds_test)\n",
        "\n",
        "all_texts = train_texts + val_texts + test_texts\n",
        "all_authors = (list(ds_train[\"author_label\"]) +\n",
        "               list(ds_val[\"author_label\"]) +\n",
        "               list(ds_test[\"author_label\"]))\n",
        "all_works = (list(ds_train[\"work\"]) +\n",
        "             list(ds_val[\"work\"]) +\n",
        "             list(ds_test[\"work\"]))\n",
        "all_splits = ([\"train\"] * len(ds_train) +\n",
        "              [\"val\"] * len(ds_val) +\n",
        "              [\"test\"] * len(ds_test))\n",
        "\n",
        "print(f\"  Total texts: {len(all_texts)}\")\n",
        "print(f\"  Train: {len(train_texts)}\")\n",
        "print(f\"  Val: {len(val_texts)}\")\n",
        "print(f\"  Test: {len(test_texts)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Step 2: Fit BERTopic Model\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nStep 2: Fitting BERTopic model...\")\n",
        "\n",
        "# Embedding model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Vectorizer (remove stopwords for better topics)\n",
        "vectorizer_model = CountVectorizer(\n",
        "    stop_words=\"english\",\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=5,\n",
        "    max_df=0.8\n",
        ")\n",
        "\n",
        "# HDBSCAN clustering\n",
        "hdbscan_model = hdbscan.HDBSCAN(\n",
        "    min_cluster_size=30,\n",
        "    min_samples=5,\n",
        "    metric=\"euclidean\",\n",
        "    cluster_selection_method=\"eom\",\n",
        "    prediction_data=True\n",
        ")\n",
        "\n",
        "# Create BERTopic model\n",
        "topic_model = BERTopic(\n",
        "    embedding_model=embedding_model,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    language=\"english\",\n",
        "    calculate_probabilities=True,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Fit and transform\n",
        "topics, probs = topic_model.fit_transform(all_texts)\n",
        "\n",
        "# Convert to numpy array for easier manipulation\n",
        "topics = np.array(topics)\n",
        "\n",
        "print(f\"\\n✓ Topic modeling complete\")\n",
        "print(f\"  Topics discovered: {len(set(topics)) - 1}\")  # Exclude -1 (outliers)\n",
        "print(f\"  Outliers: {(topics == -1).sum()} ({(topics == -1).sum()/len(topics)*100:.1f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# Step 3: Create Topic DataFrame\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nStep 3: Analyzing topic distribution...\")\n",
        "\n",
        "topic_df = pd.DataFrame({\n",
        "    'topic': topics,\n",
        "    'author': all_authors,\n",
        "    'work': all_works,\n",
        "    'split': all_splits\n",
        "})\n",
        "\n",
        "# Separate outliers and valid topics\n",
        "topic_df_outliers = topic_df[topic_df['topic'] == -1]\n",
        "topic_df_valid = topic_df[topic_df['topic'] != -1]\n",
        "\n",
        "print(f\"\\n  Valid topic assignments: {len(topic_df_valid)}\")\n",
        "print(f\"  Outliers (-1): {len(topic_df_outliers)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Step 4: Topic-Author Distribution\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TOPIC-AUTHOR DISTRIBUTION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Count topics per author\n",
        "topic_author_counts = topic_df_valid.groupby(['topic', 'author']).size().unstack(fill_value=0)\n",
        "topic_author_counts['total'] = topic_author_counts['Kant'] + topic_author_counts['Nietzsche']\n",
        "\n",
        "# Calculate balance ratio (min/max)\n",
        "topic_author_counts['balance_ratio'] = (\n",
        "    topic_author_counts[['Kant', 'Nietzsche']].min(axis=1) /\n",
        "    topic_author_counts[['Kant', 'Nietzsche']].max(axis=1)\n",
        ")\n",
        "\n",
        "# Calculate purity (how author-specific)\n",
        "topic_author_counts['purity'] = (\n",
        "    topic_author_counts[['Kant', 'Nietzsche']].max(axis=1) /\n",
        "    topic_author_counts['total']\n",
        ")\n",
        "\n",
        "# Sort by balance ratio\n",
        "topic_author_counts = topic_author_counts.sort_values('balance_ratio', ascending=False)\n",
        "\n",
        "print(f\"\\nTopic Statistics:\")\n",
        "print(f\"  Total topics: {len(topic_author_counts)}\")\n",
        "print(f\"  Mean balance ratio: {topic_author_counts['balance_ratio'].mean():.3f}\")\n",
        "print(f\"  Median balance ratio: {topic_author_counts['balance_ratio'].median():.3f}\")\n",
        "\n",
        "# Categorize topics by balance\n",
        "highly_balanced = topic_author_counts[topic_author_counts['balance_ratio'] >= 0.4]\n",
        "moderately_balanced = topic_author_counts[(topic_author_counts['balance_ratio'] >= 0.2) &\n",
        "                                          (topic_author_counts['balance_ratio'] < 0.4)]\n",
        "imbalanced = topic_author_counts[topic_author_counts['balance_ratio'] < 0.2]\n",
        "\n",
        "print(f\"\\nTopic Balance Distribution:\")\n",
        "print(f\"  Highly balanced (≥40% minority): {len(highly_balanced)} topics\")\n",
        "print(f\"  Moderately balanced (20-40%): {len(moderately_balanced)} topics\")\n",
        "print(f\"  Imbalanced (<20%): {len(imbalanced)} topics\")\n",
        "\n",
        "# Show top balanced topics\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"TOP 10 MOST BALANCED TOPICS:\")\n",
        "print(\"-\" * 80)\n",
        "print(topic_author_counts.head(10)[['Kant', 'Nietzsche', 'balance_ratio', 'purity', 'total']])\n",
        "\n",
        "# ============================================================================\n",
        "# Step 5: Examine Topic Content\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TOPIC CONTENT ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Show top 5 most balanced topics\n",
        "top_balanced = topic_author_counts.head(5)\n",
        "\n",
        "for topic_id in top_balanced.index:\n",
        "    row = topic_author_counts.loc[topic_id]\n",
        "\n",
        "    print(f\"\\nTopic {topic_id}:\")\n",
        "    print(f\"  Balance ratio: {row['balance_ratio']:.3f}\")\n",
        "    print(f\"  Kant: {row['Kant']} chunks | Nietzsche: {row['Nietzsche']} chunks\")\n",
        "\n",
        "    # Get top words\n",
        "    try:\n",
        "        top_words = topic_model.get_topic(topic_id)[:8]\n",
        "        keywords = ', '.join([word for word, score in top_words])\n",
        "        print(f\"  Keywords: {keywords}\")\n",
        "    except:\n",
        "        print(f\"  Keywords: [unavailable]\")\n",
        "\n",
        "# ============================================================================\n",
        "# Step 6: Test Set Analysis\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TEST SET TOPIC DISTRIBUTION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "test_topic_df = topic_df[topic_df['split'] == 'test']\n",
        "test_topic_df_valid = test_topic_df[test_topic_df['topic'] != -1]\n",
        "\n",
        "print(f\"\\nTest set:\")\n",
        "print(f\"  Total: {len(test_topic_df)}\")\n",
        "print(f\"  Valid topics: {len(test_topic_df_valid)}\")\n",
        "print(f\"  Outliers: {len(test_topic_df) - len(test_topic_df_valid)}\")\n",
        "\n",
        "# Topics per author in test set\n",
        "test_author_topics = test_topic_df_valid.groupby(['author', 'topic']).size().unstack(fill_value=0)\n",
        "\n",
        "print(f\"\\nTest topics per author:\")\n",
        "kant_test_topics = set(test_author_topics.loc['Kant'][test_author_topics.loc['Kant'] > 0].index)\n",
        "nietzsche_test_topics = set(test_author_topics.loc['Nietzsche'][test_author_topics.loc['Nietzsche'] > 0].index)\n",
        "shared_test_topics = kant_test_topics & nietzsche_test_topics\n",
        "\n",
        "print(f\"  Kant: {len(kant_test_topics)} unique topics\")\n",
        "print(f\"  Nietzsche: {len(nietzsche_test_topics)} unique topics\")\n",
        "print(f\"  Shared: {len(shared_test_topics)} topics\")\n",
        "print(f\"  Overlap: {shared_test_topics}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Step 7: Identify Viable Topics for H2\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"H2 VIABILITY ASSESSMENT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Criteria for H2 test:\n",
        "# 1. Balance ratio ≥ 0.3 (at least 30% from minority author)\n",
        "# 2. Total samples ≥ 20 (enough for testing)\n",
        "# 3. Present in test set\n",
        "\n",
        "BALANCE_THRESHOLD = 0.3\n",
        "MIN_SAMPLES = 20\n",
        "\n",
        "viable_topics = topic_author_counts[\n",
        "    (topic_author_counts['balance_ratio'] >= BALANCE_THRESHOLD) &\n",
        "    (topic_author_counts['total'] >= MIN_SAMPLES)\n",
        "]\n",
        "\n",
        "print(f\"\\nViable topics for H2 (balance ≥{BALANCE_THRESHOLD}, n ≥{MIN_SAMPLES}):\")\n",
        "print(f\"  Count: {len(viable_topics)}\")\n",
        "\n",
        "if len(viable_topics) > 0:\n",
        "    print(f\"\\n{viable_topics[['Kant', 'Nietzsche', 'balance_ratio', 'total']]}\")\n",
        "\n",
        "    # Check how many are in test set\n",
        "    viable_in_test = [t for t in viable_topics.index if t in shared_test_topics]\n",
        "    print(f\"\\nViable topics present in test set: {len(viable_in_test)}\")\n",
        "    print(f\"  Topic IDs: {viable_in_test}\")\n",
        "\n",
        "    # Count test samples in viable topics\n",
        "    if len(viable_in_test) > 0:\n",
        "        test_viable = test_topic_df_valid[test_topic_df_valid['topic'].isin(viable_in_test)]\n",
        "        print(f\"\\nTest samples in viable topics:\")\n",
        "        print(f\"  Total: {len(test_viable)}\")\n",
        "        print(f\"  Kant: {len(test_viable[test_viable['author'] == 'Kant'])}\")\n",
        "        print(f\"  Nietzsche: {len(test_viable[test_viable['author'] == 'Nietzsche'])}\")\n",
        "else:\n",
        "    print(\"\\n⚠️ No viable topics found for topic-based H2 control\")\n",
        "\n",
        "# ============================================================================\n",
        "# Step 8: Conclusion\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CONCLUSION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if len(viable_topics) >= 3 and len(viable_in_test) >= 2:\n",
        "    print(\"✓ TOPIC-BASED H2 CONTROL IS FEASIBLE\")\n",
        "    print(f\"  {len(viable_topics)} balanced topics available\")\n",
        "    print(f\"  {len(viable_in_test)} present in test set\")\n",
        "    print(f\"  Sufficient samples for reliable testing\")\n",
        "else:\n",
        "    print(\"✗ TOPIC-BASED H2 CONTROL NOT RECOMMENDED\")\n",
        "    print(\"\\nReasons:\")\n",
        "    if len(viable_topics) < 3:\n",
        "        print(f\"  - Only {len(viable_topics)} balanced topics (need ≥3)\")\n",
        "    if len(viable_in_test) < 2:\n",
        "        print(f\"  - Only {len(viable_in_test)} balanced topics in test set\")\n",
        "\n",
        "    print(\"\\nRecommendation:\")\n",
        "    print(\"  → Use semantic similarity approach for H2 instead\")\n",
        "    print(\"  → BERTopic reveals topic separation reflects genuine\")\n",
        "    print(\"     philosophical differences between authors\")\n",
        "\n",
        "# ============================================================================\n",
        "# Step 9: Visualization\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"GENERATING VISUALIZATIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Topic balance distribution\n",
        "axes[0, 0].hist(topic_author_counts['balance_ratio'], bins=30, edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].axvline(0.3, color='red', linestyle='--', linewidth=2, label='Viability threshold (0.3)')\n",
        "axes[0, 0].set_xlabel('Balance Ratio (min/max)')\n",
        "axes[0, 0].set_ylabel('Number of Topics')\n",
        "axes[0, 0].set_title('Topic Balance Distribution')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "# 2. Author distribution per topic (top 15 topics by size)\n",
        "top_topics = topic_author_counts.nlargest(15, 'total')\n",
        "x = np.arange(len(top_topics))\n",
        "width = 0.35\n",
        "\n",
        "axes[0, 1].bar(x - width/2, top_topics['Kant'], width, label='Kant', alpha=0.8)\n",
        "axes[0, 1].bar(x + width/2, top_topics['Nietzsche'], width, label='Nietzsche', alpha=0.8)\n",
        "axes[0, 1].set_xlabel('Topic ID')\n",
        "axes[0, 1].set_ylabel('Number of Chunks')\n",
        "axes[0, 1].set_title('Author Distribution in Top 15 Topics (by size)')\n",
        "axes[0, 1].set_xticks(x)\n",
        "axes[0, 1].set_xticklabels(top_topics.index, rotation=45)\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(alpha=0.3)\n",
        "\n",
        "# 3. Scatter: balance vs total samples\n",
        "axes[1, 0].scatter(topic_author_counts['total'],\n",
        "                   topic_author_counts['balance_ratio'],\n",
        "                   alpha=0.6, s=50)\n",
        "axes[1, 0].axhline(BALANCE_THRESHOLD, color='red', linestyle='--',\n",
        "                   label=f'Balance threshold ({BALANCE_THRESHOLD})')\n",
        "axes[1, 0].axvline(MIN_SAMPLES, color='orange', linestyle='--',\n",
        "                   label=f'Min samples ({MIN_SAMPLES})')\n",
        "axes[1, 0].set_xlabel('Total Samples in Topic')\n",
        "axes[1, 0].set_ylabel('Balance Ratio')\n",
        "axes[1, 0].set_title('Topic Balance vs Size\\n(Top-right = viable for H2)')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "# 4. Purity distribution\n",
        "axes[1, 1].hist(topic_author_counts['purity'], bins=20, edgecolor='black', alpha=0.7)\n",
        "axes[1, 1].axvline(0.8, color='red', linestyle='--', linewidth=2,\n",
        "                   label='High purity (>80% one author)')\n",
        "axes[1, 1].set_xlabel('Purity (dominant author %)')\n",
        "axes[1, 1].set_ylabel('Number of Topics')\n",
        "axes[1, 1].set_title('Topic Purity Distribution\\n(Higher = more author-specific)')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('bertopic_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Visualization saved as 'bertopic_analysis.png'\")\n",
        "\n",
        "# Save topic data for reference\n",
        "topic_author_counts.to_csv('topic_author_distribution.csv')\n",
        "print(\"✓ Topic distribution saved as 'topic_author_distribution.csv'\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:26:51.139156Z",
          "iopub.execute_input": "2026-01-02T14:26:51.139512Z",
          "iopub.status.idle": "2026-01-02T14:27:48.310330Z",
          "shell.execute_reply.started": "2026-01-02T14:26:51.139487Z",
          "shell.execute_reply": "2026-01-02T14:27:48.309480Z"
        },
        "id": "sZc3d8VXdPS_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Step 2: Filter for Viable Balanced Topics\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nStep 2: Filtering for balanced topics...\")\n",
        "\n",
        "# ✅ CORRECT - Get from BERTopic analysis\n",
        "viable_topics_in_train = topic_author_counts[\n",
        "    (topic_author_counts['balance_ratio'] >= 0.3) &\n",
        "    (topic_author_counts['total'] >= 20)\n",
        "]\n",
        "\n",
        "print(f\"\\nViable topics from BERTopic (balance ≥0.3, n≥20):\")\n",
        "print(viable_topics_in_train[['Kant', 'Nietzsche', 'balance_ratio', 'total']])\n",
        "\n",
        "# Check which viable topics are in test set\n",
        "test_topic_df = topic_df[topic_df['split'] == 'test']  # ← Filter to test set\n",
        "test_topics = test_topic_df[test_topic_df['topic'] != -1]['topic'].unique()\n",
        "viable_in_test = [t for t in viable_topics_in_train.index if t in test_topics]\n",
        "\n",
        "print(f\"\\nViable topics present in test set: {len(viable_in_test)}\")\n",
        "print(f\"  Topic IDs: {viable_in_test}\")\n",
        "\n",
        "if len(viable_in_test) == 0:\n",
        "    print(\"\\n⚠️ WARNING: No viable topics in test set!\")\n",
        "    print(\"   → Cannot perform topic-controlled evaluation\")\n",
        "    VIABLE_TOPIC_IDS = []\n",
        "else:\n",
        "    VIABLE_TOPIC_IDS = viable_in_test\n",
        "    print(f\"\\nUsing topics: {VIABLE_TOPIC_IDS}\")\n",
        "\n",
        "# Filter test set to viable topics only\n",
        "if len(VIABLE_TOPIC_IDS) > 0:\n",
        "    # Merge topic assignments with test predictions\n",
        "    test_topic_assignments = topics[len(ds_train) + len(ds_val):]  # Test set topics\n",
        "    test_pred_df['topic'] = test_topic_assignments  # Add topics to predictions\n",
        "\n",
        "    test_topic_controlled = test_pred_df[\n",
        "        test_pred_df[\"topic\"].isin(VIABLE_TOPIC_IDS)\n",
        "    ].copy()\n",
        "\n",
        "    print(f\"\\nTopic-controlled test set:\")\n",
        "    print(f\"  Topics included: {VIABLE_TOPIC_IDS}\")\n",
        "    print(f\"  Total samples: {len(test_topic_controlled)}\")\n",
        "    print(f\"\\nAuthor distribution:\")\n",
        "    print(test_topic_controlled[\"author_label\"].value_counts())\n",
        "    print(f\"\\nTopic distribution:\")\n",
        "    print(test_topic_controlled[\"topic\"].value_counts())\n",
        "else:\n",
        "    print(\"\\n✗ Cannot proceed with topic-controlled evaluation (no viable topics)\")\n",
        "    test_topic_controlled = pd.DataFrame()  # Empty dataframe"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:29:01.599923Z",
          "iopub.execute_input": "2026-01-02T14:29:01.600230Z",
          "iopub.status.idle": "2026-01-02T14:29:01.703285Z",
          "shell.execute_reply.started": "2026-01-02T14:29:01.600208Z",
          "shell.execute_reply": "2026-01-02T14:29:01.702656Z"
        },
        "id": "jry8bbp2dPTA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TOPIC-CONTROLLED RESULTS:\")\n",
        "print(f\"Total samples: {len(test_topic_controlled)}\")\n",
        "print(f\"Kant: {(test_topic_controlled['author_label']=='Kant').sum()}\")\n",
        "print(f\"Nietzsche: {(test_topic_controlled['author_label']=='Nietzsche').sum()}\")\n",
        "print(f\"Accuracy: {test_topic_controlled['correct'].mean():.3f}\")\n",
        "print(f\"Correct: {test_topic_controlled['correct'].sum()}/{len(test_topic_controlled)}\")"
      ],
      "metadata": {
        "id": "32HaeXyrq8iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "n = 32\n",
        "successes = 28\n",
        "\n",
        "# Wilson score interval\n",
        "p_hat = successes / n\n",
        "z = 1.96  # 95% confidence\n",
        "denominator = 1 + z**2/n\n",
        "center = (p_hat + z**2/(2*n)) / denominator\n",
        "margin = z * np.sqrt(p_hat*(1-p_hat)/n + z**2/(4*n**2)) / denominator\n",
        "\n",
        "ci_lower = center - margin\n",
        "ci_upper = center + margin\n",
        "\n",
        "print(f\"95% Confidence Interval: [{ci_lower:.3f}, {ci_upper:.3f}]\")"
      ],
      "metadata": {
        "id": "dpgIshHGrQ3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Semantic similarity"
      ],
      "metadata": {
        "id": "labdj2QldPTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# H2: STYLE VS SEMANTIC CONTENT CONTROL\n",
        "# ============================================================================\n",
        "# Test: Does model distinguish authors even when chunks are semantically similar\n",
        "# to opposite-author content?\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"H2: SEMANTIC SIMILARITY CONTROL\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Hypothesis: Model distinguishes authors even on semantically similar chunks\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load embedding model for semantic similarity\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Decode test and train texts\n",
        "print(\"\\nEncoding test set...\")\n",
        "test_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in ds_test[\"input_ids\"]]\n",
        "test_embeddings = embedding_model.encode(test_texts, show_progress_bar=True)\n",
        "\n",
        "print(\"Encoding train set...\")\n",
        "train_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in ds_train[\"input_ids\"]]\n",
        "train_embeddings = embedding_model.encode(train_texts, show_progress_bar=True)\n",
        "train_authors = list(ds_train[\"author_label\"])\n",
        "\n",
        "# Calculate similarity matrix\n",
        "print(\"\\nCalculating semantic similarities...\")\n",
        "similarities = cosine_similarity(test_embeddings, train_embeddings)\n",
        "\n",
        "# For each test chunk, find most semantically similar chunk from OPPOSITE author\n",
        "matched_pairs = []\n",
        "for i in range(len(test_pred_df)):\n",
        "    test_row = test_pred_df.iloc[i]\n",
        "    test_author = test_row[\"author_label\"]\n",
        "\n",
        "    # Get opposite author\n",
        "    opposite_author = \"Nietzsche\" if test_author == \"Kant\" else \"Kant\"\n",
        "    opposite_indices = [j for j, auth in enumerate(train_authors) if auth == opposite_author]\n",
        "\n",
        "    if len(opposite_indices) == 0:\n",
        "        continue\n",
        "\n",
        "    # Find most similar opposite-author chunk\n",
        "    opposite_similarities = similarities[i, opposite_indices]\n",
        "    max_similarity = np.max(opposite_similarities)\n",
        "\n",
        "    matched_pairs.append({\n",
        "        'similarity': max_similarity,\n",
        "        'true_author': test_author,\n",
        "        'predicted_author': test_row[\"pred_label\"],\n",
        "        'correct': test_row[\"correct\"]\n",
        "    })\n",
        "\n",
        "matched_df = pd.DataFrame(matched_pairs)\n",
        "\n",
        "# Analyze performance across similarity levels\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SIMILARITY DISTRIBUTION\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Mean similarity to opposite author: {matched_df['similarity'].mean():.3f}\")\n",
        "print(f\"Median similarity: {matched_df['similarity'].median():.3f}\")\n",
        "print(f\"Max similarity: {matched_df['similarity'].max():.3f}\")\n",
        "\n",
        "# Test on high semantic similarity subset\n",
        "SIMILARITY_THRESHOLD = 0.7\n",
        "semantically_controlled = matched_df[matched_df['similarity'] >= SIMILARITY_THRESHOLD]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"H2 EVALUATION: Semantic Similarity ≥ {SIMILARITY_THRESHOLD}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nSemantically-controlled test set:\")\n",
        "print(f\"  Total samples: {len(semantically_controlled)}\")\n",
        "print(f\"  Threshold: Cosine similarity ≥ {SIMILARITY_THRESHOLD}\")\n",
        "print(f\"  (These chunks are semantically similar to opposite-author content)\")\n",
        "\n",
        "if len(semantically_controlled) >= 20:  # Need reasonable sample size\n",
        "    acc_semantic = semantically_controlled['correct'].mean()\n",
        "\n",
        "    print(f\"\\nPerformance:\")\n",
        "    print(f\"  Accuracy: {acc_semantic:.3f} ({acc_semantic*100:.1f}%)\")\n",
        "    print(f\"  Correct: {semantically_controlled['correct'].sum()}/{len(semantically_controlled)}\")\n",
        "\n",
        "    # Author breakdown\n",
        "    print(f\"\\nBy true author:\")\n",
        "    for author in ['Kant', 'Nietzsche']:\n",
        "        author_subset = semantically_controlled[semantically_controlled['true_author'] == author]\n",
        "        if len(author_subset) > 0:\n",
        "            author_acc = author_subset['correct'].mean()\n",
        "            print(f\"  {author}: {author_acc:.3f} ({len(author_subset)} samples)\")\n",
        "\n",
        "    # Comparison to overall\n",
        "    overall_acc = test_pred_df['correct'].mean()\n",
        "    print(f\"\\nComparison:\")\n",
        "    print(f\"  Overall test accuracy: {overall_acc:.3f}\")\n",
        "    print(f\"  Semantic-controlled accuracy: {acc_semantic:.3f}\")\n",
        "    print(f\"  Difference: {acc_semantic - overall_acc:+.3f}\")\n",
        "\n",
        "    # H2 verdict\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"H2 INTERPRETATION\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    if acc_semantic >= 0.80:\n",
        "        print(\"✓ H2 STRONGLY SUPPORTED\")\n",
        "        print(f\"  Model achieves {acc_semantic:.1%} accuracy even when chunks are\")\n",
        "        print(f\"  semantically similar to opposite-author content\")\n",
        "        print(\"\\n  Evidence of STYLISTIC learning:\")\n",
        "        print(\"    - Syntax patterns (sentence complexity, clause structure)\")\n",
        "        print(\"    - Lexical choice (word selection, vocabulary)\")\n",
        "        print(\"    - Punctuation usage (rhetorical devices)\")\n",
        "        print(\"\\n  → Model captures genuine style beyond semantic content\")\n",
        "    elif acc_semantic >= 0.70:\n",
        "        print(\"⚠ H2 PARTIALLY SUPPORTED\")\n",
        "        print(f\"  Model achieves {acc_semantic:.1%} accuracy on semantic-controlled set\")\n",
        "        print(\"  Some stylistic signal detected, but weaker than expected\")\n",
        "    else:\n",
        "        print(\"✗ H2 NOT SUPPORTED\")\n",
        "        print(f\"  Model only achieves {acc_semantic:.1%} on semantic-controlled set\")\n",
        "        print(\"  Performance may rely more on semantic content than style\")\n",
        "else:\n",
        "    print(f\"\\n⚠️ WARNING: Only {len(semantically_controlled)} samples meet threshold\")\n",
        "    print(f\"  Try lowering similarity threshold (currently {SIMILARITY_THRESHOLD})\")\n",
        "\n",
        "    # Show distribution to help choose threshold\n",
        "    print(f\"\\nSimilarity distribution:\")\n",
        "    for threshold in [0.5, 0.55, 0.6, 0.65, 0.7]:\n",
        "        count = (matched_df['similarity'] >= threshold).sum()\n",
        "        print(f\"  ≥{threshold}: {count} samples\")\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# 1. Similarity distribution\n",
        "axes[0].hist(matched_df['similarity'], bins=50, alpha=0.7, edgecolor='black')\n",
        "axes[0].axvline(SIMILARITY_THRESHOLD, color='red', linestyle='--', linewidth=2,\n",
        "                label=f'Threshold ({SIMILARITY_THRESHOLD})')\n",
        "axes[0].set_xlabel('Cosine Similarity to Opposite Author')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Semantic Similarity Distribution\\n(Test Chunks vs Opposite-Author Training Chunks)')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# 2. Accuracy by similarity bins\n",
        "similarity_bins = pd.cut(matched_df['similarity'], bins=[0, 0.5, 0.6, 0.7, 0.8, 1.0])\n",
        "accuracy_by_bin = matched_df.groupby(similarity_bins)['correct'].agg(['mean', 'count'])\n",
        "\n",
        "bin_labels = ['0.0-0.5', '0.5-0.6', '0.6-0.7', '0.7-0.8', '0.8-1.0']\n",
        "axes[1].bar(range(len(accuracy_by_bin)), accuracy_by_bin['mean'], alpha=0.7)\n",
        "axes[1].set_xticks(range(len(accuracy_by_bin)))\n",
        "axes[1].set_xticklabels(bin_labels, rotation=45)\n",
        "axes[1].set_xlabel('Semantic Similarity Bin')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_title('Model Accuracy vs Semantic Similarity\\n(Higher similarity = more topic overlap)')\n",
        "axes[1].axhline(y=0.8, color='red', linestyle='--', label='80% threshold')\n",
        "axes[1].set_ylim([0, 1])\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "# Add sample counts\n",
        "for i, (mean, count) in enumerate(accuracy_by_bin.itertuples(index=False)):\n",
        "    if count > 0:\n",
        "        axes[1].text(i, mean + 0.02, f'n={int(count)}', ha='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('h2_semantic_similarity.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Visualization saved as 'h2_semantic_similarity.png'\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:29:05.757960Z",
          "iopub.execute_input": "2026-01-02T14:29:05.758419Z",
          "iopub.status.idle": "2026-01-02T14:29:23.411140Z",
          "shell.execute_reply.started": "2026-01-02T14:29:05.758394Z",
          "shell.execute_reply": "2026-01-02T14:29:23.410360Z"
        },
        "id": "EWNGwJBVdPTA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate accuracy vs threshold\n",
        "thresholds = np.arange(0.45, 0.71, 0.02)\n",
        "rows = []\n",
        "for t in thresholds:\n",
        "    sub = matched_df[matched_df[\"similarity\"] >= t]\n",
        "    n = len(sub)\n",
        "    acc = sub[\"correct\"].mean() if n > 0 else np.nan\n",
        "    rows.append({\"threshold\": t, \"n\": n, \"accuracy\": acc})\n",
        "\n",
        "curve = pd.DataFrame(rows).dropna()\n",
        "\n",
        "# ============================================================================\n",
        "# Create Clean, Simple Figure (single grid)\n",
        "# ============================================================================\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10, 5.5))\n",
        "\n",
        "# Primary axis: Accuracy\n",
        "ax1.plot(\n",
        "    curve[\"threshold\"], curve[\"accuracy\"],\n",
        "    marker=\"o\", markersize=7, linewidth=2.5,\n",
        "    color=\"#2563eb\", label=\"Accuracy\"\n",
        ")\n",
        "\n",
        "ax1.set_xlabel(\n",
        "    \"Minimum Cosine Similarity to Opposite-Author Training Content\",\n",
        "    fontsize=12\n",
        ")\n",
        "ax1.set_ylabel(\"Classification Accuracy\", fontsize=12, color=\"#2563eb\")\n",
        "ax1.set_ylim([0.70, 1.01])\n",
        "ax1.tick_params(axis=\"y\", labelcolor=\"#2563eb\")\n",
        "ax1.tick_params(axis=\"both\", labelsize=10)\n",
        "\n",
        "# SINGLE grid (anchor everything to accuracy axis)\n",
        "ax1.grid(\n",
        "    True,\n",
        "    which=\"major\",\n",
        "    axis=\"both\",\n",
        "    alpha=0.25,\n",
        "    linestyle=\"-\",\n",
        "    linewidth=0.6,\n",
        "    color=\"gray\"\n",
        ")\n",
        "ax1.set_axisbelow(True)\n",
        "\n",
        "# Secondary axis: Sample size (NO grid)\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(\n",
        "    curve[\"threshold\"], curve[\"n\"],\n",
        "    marker=\"s\", markersize=5, linewidth=2,\n",
        "    linestyle=\"--\", color=\"#6b7280\",\n",
        "    label=\"Sample Size\", alpha=0.7\n",
        ")\n",
        "\n",
        "ax2.set_ylabel(\"Subset Size (n)\", fontsize=12, color=\"#6b7280\")\n",
        "ax2.tick_params(axis=\"y\", labelcolor=\"#6b7280\")\n",
        "\n",
        "\n",
        "\n",
        "# Explicitly disable secondary grid\n",
        "ax2.grid(False)\n",
        "\n",
        "# Title\n",
        "plt.title(\n",
        "    \"Model Performance Under Increasing Semantic Overlap\",\n",
        "    fontsize=13, pad=14, fontweight=\"600\"\n",
        ")\n",
        "\n",
        "# Combined legend\n",
        "lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(\n",
        "    lines1 + lines2,\n",
        "    labels1 + labels2,\n",
        "    loc=\"lower left\",\n",
        "    fontsize=10,\n",
        "    frameon=True,\n",
        "    edgecolor=\"gray\",\n",
        "    framealpha=0.95\n",
        ")\n",
        "\n",
        "# Clean spines\n",
        "for spine in [\"top\"]:\n",
        "    ax1.spines[spine].set_visible(False)\n",
        "    ax2.spines[spine].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "plt.savefig(\"outputs/h2_similarity_curve.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Saved: outputs/h2_similarity_curve.png\")"
      ],
      "metadata": {
        "id": "DAM9Zb4EarLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LIME"
      ],
      "metadata": {
        "id": "hKpQM19IdPTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MODEL INTERPRETABILITY - LIME ANALYSIS\n",
        "# ============================================================================\n",
        "# Goal: Identify which linguistic features drive model predictions\n",
        "# Method: LIME (Local Interpretable Model-agnostic Explanations)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"H3: MODEL INTERPRETABILITY\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Method: LIME (Local Interpretable Model-agnostic Explanations)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# Step 1: Install and Import LIME\n",
        "# ============================================================================\n",
        "\n",
        "!pip install -q lime\n",
        "\n",
        "import lime\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"\\n✓ LIME installed and imported\")\n",
        "\n",
        "# ============================================================================\n",
        "# Step 2: Create Prediction Function for LIME\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nStep 2: Setting up prediction function...\")\n",
        "\n",
        "# LIME needs a function that takes text strings and returns probabilities\n",
        "def predict_proba(texts):\n",
        "    \"\"\"\n",
        "    Predict probabilities for a list of text strings.\n",
        "    Returns: numpy array of shape (n_texts, 2) with probabilities for [Kant, Nietzsche]\n",
        "    \"\"\"\n",
        "    # Tokenize texts\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    # Move to device\n",
        "    inputs = {k: v.to(best_model.device) for k, v in inputs.items()}\n",
        "\n",
        "    # Get predictions\n",
        "    best_model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = best_model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "    # Return as numpy array\n",
        "    return probs.cpu().numpy()\n",
        "\n",
        "# Test the function\n",
        "test_text = \"Thus spoke Zarathustra to the people.\"\n",
        "test_probs = predict_proba([test_text])\n",
        "print(f\"\\n✓ Prediction function working\")\n",
        "print(f\"  Test probabilities: Kant={test_probs[0][0]:.3f}, Nietzsche={test_probs[0][1]:.3f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Step 3: Initialize LIME Explainer\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nStep 3: Initializing LIME explainer...\")\n",
        "\n",
        "# Create LIME explainer\n",
        "explainer = LimeTextExplainer(\n",
        "    class_names=['Kant', 'Nietzsche'],\n",
        "    split_expression=r'\\W+',  # Split on non-word characters\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"✓ LIME explainer initialized\")\n",
        "\n",
        "# ============================================================================\n",
        "# Step 4: Select Representative Test Samples\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SELECTING REPRESENTATIVE SAMPLES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Select samples for explanation\n",
        "# Strategy: Get high-confidence correct predictions for each author\n",
        "\n",
        "# Correct predictions with high confidence\n",
        "test_correct = test_pred_df[test_pred_df['correct'] == True].copy()\n",
        "\n",
        "# High confidence Kant predictions (correctly classified)\n",
        "kant_correct = test_correct[test_correct['author_label'] == 'Kant'].nlargest(5, 'p_kant')\n",
        "# High confidence Nietzsche predictions (correctly classified)\n",
        "nietzsche_correct = test_correct[test_correct['author_label'] == 'Nietzsche'].nlargest(5, 'p_nietzsche')\n",
        "\n",
        "# Also get misclassifications (interesting edge cases)\n",
        "test_incorrect = test_pred_df[test_pred_df['correct'] == False].copy()\n",
        "\n",
        "print(f\"\\nSelected samples:\")\n",
        "print(f\"  Kant (correct, high confidence): {len(kant_correct)}\")\n",
        "print(f\"  Nietzsche (correct, high confidence): {len(nietzsche_correct)}\")\n",
        "print(f\"  Misclassified: {len(test_incorrect)}\")\n",
        "\n",
        "# Combine into analysis set\n",
        "samples_to_explain = pd.concat([\n",
        "    kant_correct.head(3),\n",
        "    nietzsche_correct.head(3),\n",
        "    test_incorrect.head(2) if len(test_incorrect) > 0 else pd.DataFrame()\n",
        "], ignore_index=True)\n",
        "\n",
        "print(f\"\\nTotal samples for LIME analysis: {len(samples_to_explain)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Step 5: Generate LIME Explanations\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"GENERATING LIME EXPLANATIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "explanations = []\n",
        "\n",
        "for idx, row in samples_to_explain.iterrows():\n",
        "    # Get the text\n",
        "    chunk_idx = row.name if hasattr(row, 'name') else idx\n",
        "    text = tokenizer.decode(ds_test[chunk_idx][\"input_ids\"], skip_special_tokens=True)\n",
        "\n",
        "    true_label = row['author_label']\n",
        "    pred_label = row['pred_label']\n",
        "    confidence = row['p_kant'] if pred_label == 'Kant' else row['p_nietzsche']\n",
        "\n",
        "    print(f\"\\n[{idx+1}/{len(samples_to_explain)}] Explaining: {true_label} → Predicted: {pred_label} (conf: {confidence:.3f})\")\n",
        "\n",
        "    # Generate explanation\n",
        "    exp = explainer.explain_instance(\n",
        "        text,\n",
        "        predict_proba,\n",
        "        num_features=10,  # Top 10 most important words\n",
        "        num_samples=500   # Number of perturbed samples\n",
        "    )\n",
        "\n",
        "    explanations.append({\n",
        "        'index': chunk_idx,\n",
        "        'text': text,\n",
        "        'true_label': true_label,\n",
        "        'pred_label': pred_label,\n",
        "        'confidence': confidence,\n",
        "        'correct': row['correct'],\n",
        "        'explanation': exp,\n",
        "        'top_features': exp.as_list()\n",
        "    })\n",
        "\n",
        "    print(f\"  Top features: {[f[0] for f in exp.as_list()[:5]]}\")\n",
        "\n",
        "print(f\"\\n✓ Generated {len(explanations)} LIME explanations\")\n",
        "\n",
        "# ============================================================================\n",
        "# Step 6: Analyze Feature Importance Patterns\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Aggregate features across all explanations\n",
        "all_features = []\n",
        "\n",
        "for exp_data in explanations:\n",
        "    true_label = exp_data['true_label']\n",
        "    pred_label = exp_data['pred_label']\n",
        "\n",
        "    for word, weight in exp_data['top_features']:\n",
        "        all_features.append({\n",
        "            'word': word,\n",
        "            'weight': weight,\n",
        "            'true_label': true_label,\n",
        "            'pred_label': pred_label,\n",
        "            'correct': exp_data['correct'],\n",
        "            # Weight direction: positive = supports Nietzsche, negative = supports Kant\n",
        "            'supports': 'Nietzsche' if weight > 0 else 'Kant'\n",
        "        })\n",
        "\n",
        "features_df = pd.DataFrame(all_features)\n",
        "\n",
        "print(f\"\\nTotal feature attributions extracted: {len(features_df)}\")\n",
        "\n",
        "# Most important features for each author\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"TOP FEATURES SUPPORTING KANT (Negative weights):\")\n",
        "print(\"-\" * 80)\n",
        "kant_features = features_df[features_df['weight'] < 0].groupby('word')['weight'].agg(['mean', 'count'])\n",
        "kant_features = kant_features[kant_features['count'] >= 2].sort_values('mean').head(15)\n",
        "print(kant_features)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"TOP FEATURES SUPPORTING NIETZSCHE (Positive weights):\")\n",
        "print(\"-\" * 80)\n",
        "nietzsche_features = features_df[features_df['weight'] > 0].groupby('word')['weight'].agg(['mean', 'count'])\n",
        "nietzsche_features = nietzsche_features[nietzsche_features['count'] >= 2].sort_values('mean', ascending=False).head(15)\n",
        "print(nietzsche_features)\n",
        "\n",
        "# ============================================================================\n",
        "# Step 7: Visualize LIME Explanations\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"VISUALIZING LIME EXPLANATIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create figure with multiple subplots\n",
        "n_examples = min(4, len(explanations))\n",
        "fig, axes = plt.subplots(n_examples, 1, figsize=(14, 4*n_examples))\n",
        "\n",
        "if n_examples == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for i in range(n_examples):\n",
        "    exp_data = explanations[i]\n",
        "    features = exp_data['top_features'][:10]\n",
        "\n",
        "    words = [f[0] for f in features]\n",
        "    weights = [f[1] for f in features]\n",
        "    colors = ['#d62728' if w < 0 else '#2ca02c' for w in weights]\n",
        "\n",
        "    axes[i].barh(range(len(words)), weights, color=colors, alpha=0.7)\n",
        "    axes[i].set_yticks(range(len(words)))\n",
        "    axes[i].set_yticklabels(words)\n",
        "    axes[i].axvline(0, color='black', linestyle='-', linewidth=0.8)\n",
        "    axes[i].set_xlabel('Feature Weight (← Kant | Nietzsche →)')\n",
        "\n",
        "    title = f\"{exp_data['true_label']} → Predicted: {exp_data['pred_label']} \"\n",
        "    title += f\"({'✓ Correct' if exp_data['correct'] else '✗ Wrong'})\"\n",
        "    axes[i].set_title(title, fontsize=12, fontweight='bold')\n",
        "    axes[i].grid(alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('h3_lime_explanations.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Visualization saved as 'h3_lime_explanations.png'\")\n",
        "\n",
        "# ============================================================================\n",
        "# Step 8: Display Interactive HTML Explanations\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"GENERATING HTML EXPLANATIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save HTML visualizations for each explanation\n",
        "for i, exp_data in enumerate(explanations[:3]):  # Show first 3\n",
        "    html = exp_data['explanation'].as_html()\n",
        "\n",
        "    filename = f\"lime_explanation_{i+1}_{exp_data['true_label']}.html\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(html)\n",
        "\n",
        "    print(f\"✓ Saved: {filename}\")\n",
        "\n",
        "    # Display in notebook (if in Jupyter)\n",
        "    from IPython.display import HTML, display\n",
        "    print(f\"\\nExample {i+1}: {exp_data['true_label']} (predicted: {exp_data['pred_label']})\")\n",
        "    print(f\"Text preview: {exp_data['text'][:200]}...\")\n",
        "    print(f\"\\nTop features:\")\n",
        "    for word, weight in exp_data['top_features'][:5]:\n",
        "        direction = \"→ Nietzsche\" if weight > 0 else \"→ Kant\"\n",
        "        print(f\"  '{word}': {weight:+.3f} {direction}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Step 9: Statistical Summary\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"H3 STATISTICAL SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Feature frequency analysis\n",
        "print(\"\\nFeature Attribution Statistics:\")\n",
        "print(f\"  Total attributions: {len(features_df)}\")\n",
        "print(f\"  Unique words: {features_df['word'].nunique()}\")\n",
        "print(f\"  Mean |weight|: {features_df['weight'].abs().mean():.3f}\")\n",
        "print(f\"  Median |weight|: {features_df['weight'].abs().median():.3f}\")\n",
        "\n",
        "# Compare correct vs incorrect predictions\n",
        "if len(test_incorrect) > 0:\n",
        "    print(\"\\nFeature patterns by prediction correctness:\")\n",
        "    correct_weights = features_df[features_df['correct'] == True]['weight'].abs().mean()\n",
        "    incorrect_weights = features_df[features_df['correct'] == False]['weight'].abs().mean()\n",
        "    print(f\"  Mean |weight| (correct): {correct_weights:.3f}\")\n",
        "    print(f\"  Mean |weight| (incorrect): {incorrect_weights:.3f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# Step 10: Linguistic Pattern Analysis\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"LINGUISTIC PATTERN ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Categorize features by type\n",
        "def categorize_feature(word):\n",
        "    \"\"\"Categorize words into linguistic types\"\"\"\n",
        "    # Punctuation\n",
        "    if word in ['.', ',', ';', ':', '!', '?', '--', '—']:\n",
        "        return 'punctuation'\n",
        "    # Common function words\n",
        "    elif word.lower() in ['the', 'a', 'an', 'is', 'be', 'to', 'of', 'and', 'in', 'that', 'it']:\n",
        "        return 'function_word'\n",
        "    # Philosophical terms\n",
        "    elif word.lower() in ['reason', 'kant', 'nietzsche', 'thus', 'therefore', 'moral', 'virtue', 'knowledge']:\n",
        "        return 'philosophical_term'\n",
        "    # Length-based\n",
        "    elif len(word) > 10:\n",
        "        return 'long_word'\n",
        "    elif len(word) <= 3:\n",
        "        return 'short_word'\n",
        "    else:\n",
        "        return 'content_word'\n",
        "\n",
        "features_df['category'] = features_df['word'].apply(categorize_feature)\n",
        "\n",
        "print(\"\\nFeature distribution by linguistic category:\")\n",
        "category_dist = features_df.groupby(['category', 'supports']).size().unstack(fill_value=0)\n",
        "print(category_dist)\n",
        "\n",
        "print(\"\\nAverage weights by category:\")\n",
        "category_weights = features_df.groupby('category')['weight'].agg(['mean', 'count'])\n",
        "print(category_weights.sort_values('mean', key=abs, ascending=False))\n",
        "\n",
        "# ============================================================================\n",
        "# Step 11: H3 Interpretation\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"H3 INTERPRETATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Identify strongest patterns\n",
        "top_kant_words = kant_features.head(5).index.tolist()\n",
        "top_nietzsche_words = nietzsche_features.head(5).index.tolist()\n",
        "\n",
        "print(\"\\n✓ H3: MODEL INTERPRETABILITY FINDINGS\")\n",
        "print(\"\\nLIME attribution analysis reveals interpretable features:\")\n",
        "\n",
        "print(\"\\nStrongest Kant indicators:\")\n",
        "for word in top_kant_words:\n",
        "    weight = kant_features.loc[word, 'mean']\n",
        "    count = kant_features.loc[word, 'count']\n",
        "    print(f\"  • '{word}': {weight:.3f} (appears {int(count)} times)\")\n",
        "\n",
        "print(\"\\nStrongest Nietzsche indicators:\")\n",
        "for word in top_nietzsche_words:\n",
        "    weight = nietzsche_features.loc[word, 'mean']\n",
        "    count = nietzsche_features.loc[word, 'count']\n",
        "    print(f\"  • '{word}': {weight:.3f} (appears {int(count)} times)\")\n",
        "\n",
        "print(\"\\nLinguistic Interpretation:\")\n",
        "print(\"  The model attends to both lexical and structural features:\")\n",
        "print(\"    - Vocabulary preferences (philosophical terminology)\")\n",
        "print(\"    - Syntactic markers (function words, sentence connectors)\")\n",
        "print(\"    - Stylistic elements (punctuation, sentence complexity)\")\n",
        "print(\"\\n  These patterns align with known stylistic differences:\")\n",
        "print(\"    - Kant: Formal, systematic, abstract vocabulary\")\n",
        "print(\"    - Nietzsche: Direct, provocative, aphoristic style\")\n",
        "\n",
        "# ============================================================================\n",
        "# Step 12: Save Results\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SAVING RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save feature importance table\n",
        "features_summary = features_df.groupby('word').agg({\n",
        "    'weight': ['mean', 'std', 'count'],\n",
        "    'supports': lambda x: x.mode()[0] if len(x) > 0 else None\n",
        "}).round(3)\n",
        "features_summary.columns = ['mean_weight', 'std_weight', 'count', 'primary_support']\n",
        "features_summary = features_summary.sort_values('mean_weight', key=abs, ascending=False)\n",
        "features_summary.to_csv('h3_lime_feature_importance.csv')\n",
        "\n",
        "print(\"✓ Saved: h3_lime_feature_importance.csv\")\n",
        "\n",
        "# Save individual explanations\n",
        "explanations_summary = pd.DataFrame([{\n",
        "    'index': e['index'],\n",
        "    'true_label': e['true_label'],\n",
        "    'pred_label': e['pred_label'],\n",
        "    'confidence': e['confidence'],\n",
        "    'correct': e['correct'],\n",
        "    'top_features': ', '.join([f\"{w}({s:+.2f})\" for w, s in e['top_features'][:5]])\n",
        "} for e in explanations])\n",
        "\n",
        "explanations_summary.to_csv('h3_lime_explanations_summary.csv', index=False)\n",
        "print(\"✓ Saved: h3_lime_explanations_summary.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"H3 ANALYSIS COMPLETE\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nFiles generated:\")\n",
        "print(\"  1. h3_lime_explanations.png - Feature weight visualizations\")\n",
        "print(\"  2. lime_explanation_*.html - Interactive HTML explanations\")\n",
        "print(\"  3. h3_lime_feature_importance.csv - Aggregated feature weights\")\n",
        "print(\"  4. h3_lime_explanations_summary.csv - Summary of all explanations\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-02T14:29:26.743782Z",
          "iopub.execute_input": "2026-01-02T14:29:26.744072Z",
          "iopub.status.idle": "2026-01-02T14:29:47.121525Z",
          "shell.execute_reply.started": "2026-01-02T14:29:26.744051Z",
          "shell.execute_reply": "2026-01-02T14:29:47.120801Z"
        },
        "id": "fr-KJA4gdPTB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CONDENSED VISUALIZATION: Aggregated LIME Features\n",
        "# ============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Collect all features from all explanations\n",
        "all_features = []\n",
        "for exp in explanations:\n",
        "    for word, weight in exp['top_features']:\n",
        "        all_features.append({\n",
        "            'word': word,\n",
        "            'weight': weight\n",
        "        })\n",
        "\n",
        "features_df = pd.DataFrame(all_features)\n",
        "\n",
        "# Aggregate by word: mean weight and frequency\n",
        "feature_summary = features_df.groupby('word').agg({\n",
        "    'weight': ['mean', 'count']\n",
        "}).reset_index()\n",
        "feature_summary.columns = ['word', 'mean_weight', 'count']\n",
        "\n",
        "# Filter: only words appearing at least 2 times\n",
        "feature_summary = feature_summary[feature_summary['count'] >= 2]\n",
        "\n",
        "# Sort by absolute weight and take top 15\n",
        "feature_summary['abs_weight'] = feature_summary['mean_weight'].abs()\n",
        "top_features = feature_summary.nlargest(20, 'abs_weight').sort_values('mean_weight')\n",
        "\n",
        "# ============================================================================\n",
        "# Create Figure\n",
        "# ============================================================================\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "words = top_features['word'].values\n",
        "weights = top_features['mean_weight'].values\n",
        "counts = top_features['count'].values\n",
        "\n",
        "# Color: red for Kant (negative), green for Nietzsche (positive)\n",
        "colors = ['#d62728' if w < 0 else '#2ca02c' for w in weights]\n",
        "\n",
        "# Horizontal bars\n",
        "bars = ax.barh(range(len(words)), weights, color=colors, alpha=0.75,\n",
        "               edgecolor='black', linewidth=1.2)\n",
        "\n",
        "# Add frequency annotations\n",
        "for i, (weight, count) in enumerate(zip(weights, counts)):\n",
        "    ax.text(weight + (0.001 if weight > 0 else -0.001), i,\n",
        "            f'n={int(count)}',\n",
        "            ha='left' if weight > 0 else 'right',\n",
        "            va='center', fontsize=9, style='italic')\n",
        "\n",
        "# Y-axis\n",
        "ax.set_yticks(range(len(words)))\n",
        "ax.set_yticklabels(words, fontsize=11)\n",
        "\n",
        "# X-axis\n",
        "ax.axvline(0, color='black', linewidth=1.5)\n",
        "ax.set_xlabel('Mean Feature Weight Across All Explanations',\n",
        "              fontsize=12, fontweight='bold')\n",
        "ax.set_xlim([min(weights)*1.3, max(weights)*1.3])\n",
        "\n",
        "# Title\n",
        "ax.set_title('LIME Feature Attribution: Aggregated Across All Explained Chunks\\n'\n",
        "             '(n=8 chunks, 80 total attributions)',\n",
        "             fontsize=13, fontweight='bold', pad=15)\n",
        "\n",
        "# Legend\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [\n",
        "    Patch(facecolor='#d62728', alpha=0.75, label='← Supports Kant'),\n",
        "    Patch(facecolor='#2ca02c', alpha=0.75, label='→ Supports Nietzsche')\n",
        "]\n",
        "ax.legend(handles=legend_elements, loc='lower right', fontsize=10, framealpha=0.9)\n",
        "\n",
        "# Grid\n",
        "ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "ax.set_axisbelow(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('lime_aggregated_features.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Saved: lime_aggregated_features.png\")\n",
        "print(f\"\\nShowing top 15 features that appear ≥2 times\")\n",
        "print(f\"Total unique words: {len(feature_summary)}\")\n",
        "print(f\"Total attributions: {len(features_df)}\")"
      ],
      "metadata": {
        "id": "qTq0b0r6f7mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# AGGREGATED FEATURE ANALYSIS (All Explanations)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"AGGREGATED LIME ANALYSIS (All Explained Chunks)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Collect all features across all explanations\n",
        "all_features = []\n",
        "for exp in explanations:\n",
        "    for word, weight in exp['top_features']:\n",
        "        all_features.append({\n",
        "            'word': word,\n",
        "            'weight': weight,\n",
        "            'true_label': exp['true_label'],\n",
        "            'correct': exp['correct']\n",
        "        })\n",
        "\n",
        "features_df = pd.DataFrame(all_features)\n",
        "\n",
        "# Most consistent Kant indicators (negative weights)\n",
        "kant_features = features_df[features_df['weight'] < 0].groupby('word')['weight'].agg(['mean', 'count'])\n",
        "kant_features = kant_features[kant_features['count'] >= 2].sort_values('mean').head(10)\n",
        "\n",
        "print(\"\\nTop 10 Consistent Kant Indicators:\")\n",
        "print(kant_features)\n",
        "\n",
        "# Most consistent Nietzsche indicators (positive weights)\n",
        "nietzsche_features = features_df[features_df['weight'] > 0].groupby('word')['weight'].agg(['mean', 'count'])\n",
        "nietzsche_features = nietzsche_features[nietzsche_features['count'] >= 2].sort_values('mean', ascending=False).head(10)\n",
        "\n",
        "print(\"\\nTop 10 Consistent Nietzsche Indicators:\")\n",
        "print(nietzsche_features)\n",
        "\n",
        "# Categorize by type\n",
        "stopwords_set = {'the', 'a', 'an', 'is', 'of', 'in', 'to', 'and', 'or', 'but',\n",
        "                 'we', 'it', 'however', 'thus', 'therefore', 'such', 'which'}\n",
        "\n",
        "def categorize(word):\n",
        "    if word.lower() in stopwords_set:\n",
        "        return 'Function Word'\n",
        "    elif len(word) <= 3:\n",
        "        return 'Short Word'\n",
        "    else:\n",
        "        return 'Content Word'\n",
        "\n",
        "features_df['category'] = features_df['word'].apply(categorize)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FEATURE DISTRIBUTION BY TYPE\")\n",
        "print(\"=\" * 80)\n",
        "print(features_df.groupby(['category', 'true_label']).size().unstack(fill_value=0))\n",
        "\n",
        "print(\"\\n✓ Interpretation: Both function words (style) and content words (lexical)\")\n",
        "print(\"  contribute to authorship attribution. This is expected and desirable.\")"
      ],
      "metadata": {
        "id": "_djpEy6TuoLR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}